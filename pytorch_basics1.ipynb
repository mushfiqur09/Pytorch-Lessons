{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cab714e7-e195-42b5-ad39-9d2a70cebdf5",
   "metadata": {},
   "source": [
    "# Pytorch basics\n",
    "Main target is to go through the tutorials from [here](https://www.youtube.com/playlist?list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fffa6bde-edc4-4c99-84ef-85e8adaffb70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419becc7-c989-48f3-a3d8-de4953b45cf2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tutorial 2: Tensor objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "36573478-3049-42de-84a9-f82ae2335671",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ways to define torch objects\n",
    "x = torch.empty(2, 2, dtype = torch.int)\n",
    "y = torch.rand(2, 2, dtype = torch.float)\n",
    "z = torch.zeros(2, 2)\n",
    "z  = torch.ones(2, 2)\n",
    "a = torch.tensor([[2,2], [3,3]], dtype = torch.float)\n",
    "\n",
    "# I can do all these operations now\n",
    "z = x + a\n",
    "z = torch.add(x, a)\n",
    "z = torch.sub(x, a)\n",
    "z = torch.mul(x, a)\n",
    "z = torch.div(x, a)\n",
    "\n",
    "# slicing, reshaping data\n",
    "x[0, :]\n",
    "x.view(4) \n",
    "x.view(-1, 1)\n",
    "x.reshape(1, 4)\n",
    "\n",
    "# juggling between numpy and torch\n",
    "z = z.numpy(); # print(type(z))\n",
    "z = torch.from_numpy(z)\n",
    "\n",
    "# A special case where we need a variable who's gradients need to be calculated\n",
    "w = torch.ones(5, requires_grad = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1f1708-3db1-474b-bb59-96aa5a1db156",
   "metadata": {},
   "source": [
    "Recap:\n",
    "* numpy, torch -> libraries/ packages (inside that there'll be classes, functions, datatypes defined)\n",
    "* x = torch.zeros(2) creates a torch object named x; I can see that using type(x)\n",
    "* x.dtype will show the data type of the torch object x\n",
    "* you'll notice sometimes we do x.type and sometimes x.size(). This is because type is an attribute of x and size is a function defined under that object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fa039b9b-0573-42a9-977e-2e31c9a3964f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sometimes if CUDA is available, it can accelerate a lot of computation. Looks like for my case, it is not but keeping the codes anyways\n",
    "if torch.cuda.is_available():\n",
    "    print('cuda available')\n",
    "    device = torch.device(\"cuda\")\n",
    "    x = torch.ones(5, device = device)\n",
    "    y = torch.ones(5);\n",
    "    y = y.to(device);\n",
    "    z = x + y # is faster because they are both in GPU\n",
    "    # careful: z.numpy() cannot be done now cause numpy cannot work in GPU\n",
    "    z = z.to(\"cpu\")\n",
    "    # Check number of CUDA devices available\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031a4644-adef-4ef1-a7a3-81c1dc85cccf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tutorial 3: Gradient Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0ab5b3bf-2bf8-4b0e-a042-e61eecf59a20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = torch.randn(3, requires_grad = True) # print(x) shows the flag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4ba47ae5-c7dd-4667-9b18-b4f6215a683f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.9973, 1.6517, 1.1172], grad_fn=<AddBackward0>)\n",
      "tensor(8.6401, grad_fn=<MeanBackward0>)\n",
      "tensor([24.3784, 25.3258,  8.2014])\n"
     ]
    }
   ],
   "source": [
    "y = x + 2; # Think of y as the output neuron and x and 2 are the input neurons. \n",
    "# If the RHS has requires_grad = True set, pytorch will automatically create an attribute grad_fn which is a function that can calculate dy/dx\n",
    "print(y)\n",
    "z = y * y * 2; \n",
    "z = z.mean(); \n",
    "print(z)\n",
    "z.backward() # dz / dx # mean opeartion is necessary here because if z is a vector then it's a mess.\n",
    "\n",
    "z = y * y * 2; # if z is not a scalar\n",
    "v = torch.tensor([0.1, 1.0, .001], dtype = torch.float32); # 3d cause x is also 3d\n",
    "z.backward(v)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c1cdfb-d577-46ab-ad07-087eefcbdb73",
   "metadata": {},
   "source": [
    "$Notice$ : z.backward() does not work for \n",
    "* vector z. I didn't understand how the vector v in the previous code was defined\n",
    "* if I don't specify requires_grad = true\n",
    "\n",
    "So, the part that fascillitates calculation of gradient to every new variable that are dependent on the original is the \"requires_grad = True\" command. \n",
    "This can be potentially dangerous as if we define a new varaible that depends on it, the new variable will start becoming a part of the gradients. \n",
    "To detach the variable from gradients there are three options: \n",
    "\n",
    "```python \n",
    "x.requires_grad_(False)\n",
    "x.detach()\n",
    "with torch.no_grad():\n",
    "    {\n",
    "        y = ...\n",
    "    }\n",
    "```\n",
    "\n",
    "$Important$: *.grad keeps accumulating. So, we must empty the grad varaibles if we don't want it. The way to do that is by adding ```weights.grad.zero_()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f5fb5e96-7365-4a50-9ba6-a64d87ea6bf4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([6., 6., 6., 6.])\n",
      "tensor([9., 9., 9., 9.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad = True);\n",
    "for epoch in range(3):\n",
    "    model_output = (weights * 3).sum();\n",
    "    model_output.backward()\n",
    "    print(weights.grad)\n",
    "    #weights.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c2484c-0a4d-42b1-9593-84cc11c5af74",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tutorial 4: Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6091a80-0b7c-40ed-a122-10ad46859596",
   "metadata": {},
   "source": [
    "$\\textbf{Chain rule:}$ x -> y = a(x) -> z = b(y); and we want to know \n",
    "\n",
    "dz/dx = dz/dy. dy/dx.\n",
    "\n",
    "We use this to perform backpropagation in three steps: \n",
    "1. Forward pass\n",
    "2. Compute the loss\n",
    "3. backward pass & update\n",
    "\n",
    "See the image at [this](https://www.youtube.com/watch?v=3Kb0QS6z7WA&list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&index=4) link (5:35) for a better understanding of the variables w, x, y, yhat, s, L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e066a7e1-4c14-4b9a-b7a3-bc8dc1ec31f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "w = torch.tensor(1.0, requires_grad = True)\n",
    "\n",
    "yHat = w * x # forward pass\n",
    "\n",
    "L = (yHat - y)**2\n",
    "\n",
    "L.backward(); # backward pass\n",
    "\n",
    "print(w.grad) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7eb1aca-6f7b-420a-a2b8-60f65c1aea64",
   "metadata": {},
   "source": [
    "So, backpropagation is as simple as just typing L.backward()\n",
    "and calculating gradient is also just parameter.grad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f99f827-00ca-4f54-adb9-08e37abfd6cd",
   "metadata": {},
   "source": [
    "### Tutorial 5: GD with autograd and Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9b360a-d1fd-4bdf-837f-a0e7adf21f69",
   "metadata": {},
   "source": [
    "Try 1: Let's do it manually first to really appreciate what pytorch is doing for us. \n",
    "\n",
    "X = numpy.array([1, 2, 3, 4], dtype = \"float32\") doesn't work due to some formatting issue. See [this](https://stackoverflow.com/questions/43911844/numpy-float32-gives-different-value-from-dtype-float32-in-array) for details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "2fd17948-76a5-4290-af10-e097e70c13b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training : f(5) = 0\n",
      "epoch 1: w = 1.200, loss = 30.000\n",
      "epoch 21: w = 2.000, loss = 0.000\n",
      "epoch 41: w = 2.000, loss = 0.000\n",
      "epoch 61: w = 2.000, loss = 0.000\n",
      "epoch 81: w = 2.000, loss = 0.000\n",
      "Prediction after training: f(5) = 9.99999977350235\n"
     ]
    }
   ],
   "source": [
    "# linear regression example (f = w* x). That is, we'll try to fit a set of data to this formula.\n",
    "X = numpy.array([1, 2, 3, 4], dtype = numpy.float32);\n",
    "Y = numpy.array([2, 4, 6, 8], dtype = numpy.float32); \"\"\" these are our input data. we'll create a model that fits this data. That is find the optimum w\"\"\"\n",
    "\n",
    "w = 0 # wguess\n",
    "# model\n",
    "def forward(x):\n",
    "    return w * x;\n",
    "\n",
    "# loss\n",
    "def loss(y, yHat):\n",
    "    return ((yHat - y)**2).mean();\n",
    "\n",
    "# gradient\n",
    "def gradient(x, y, yHat):\n",
    "    return numpy.dot(2 * x, yHat - y).mean(); \n",
    "print(f'Prediction before training : f(5) = {forward(5)}');\n",
    "\n",
    "# Training\n",
    "learningRate = 0.01\n",
    "nIters = 100;\n",
    "\n",
    "for epoch in range(nIters):\n",
    "    YHat = forward(X);\n",
    "    L = loss(Y, YHat);\n",
    "    dw = gradient(X, Y, YHat);\n",
    "    \n",
    "    # update weights\n",
    "    w -= learningRate * dw\n",
    "    \n",
    "    if epoch % 20 ==0:\n",
    "        print(f'epoch {epoch + 1}: w = {w:.3f}, loss = {L:0.3f}');\n",
    "print(f'Prediction after training: f(5) = {forward(5)}');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b253a1-32ae-4266-ba6f-264ea3f9b229",
   "metadata": {
    "tags": []
   },
   "source": [
    "Try 2: pytorch (calculate gradient automatically)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "3e520ece-1a11-4a59-83a6-6f9b6f0d4eaf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training : f(5) = 0.0\n",
      "epoch 1: w = 0.300, loss = 30.000\n",
      "epoch 21: w = 1.934, loss = 0.045\n",
      "epoch 41: w = 1.997, loss = 0.000\n",
      "epoch 61: w = 2.000, loss = 0.000\n",
      "epoch 81: w = 2.000, loss = 0.000\n",
      "Prediction after training: f(5) = 9.999998092651367\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor([1, 2, 3, 4], dtype = torch.float32);\n",
    "Y = torch.tensor([2, 4, 6, 8], dtype = torch.float32); \"\"\" these are our input data. we'll create a model that fits this data. That is find the optimum w\"\"\"\n",
    "\n",
    "w = torch.tensor(0.0, dtype = torch.float32, requires_grad = True) # wguess\n",
    "\n",
    "# model (same as before)\n",
    "def forward(x):\n",
    "    return w * x;\n",
    " \n",
    "# loss (same as before)\n",
    "def loss(y, yHat):\n",
    "    return ((yHat - y)**2).mean();\n",
    "\n",
    "# gradient -> no need to write an explicit function!\n",
    "\n",
    "# Training\n",
    "learningRate = 0.01\n",
    "nIters = 100;\n",
    "\n",
    "print(f'Prediction before training : f(5) = {forward(5)}');\n",
    "for epoch in range(nIters):\n",
    "    YHat = forward(X);\n",
    "    \n",
    "    L = loss(Y, YHat);\n",
    "    \n",
    "    L.backward(); \n",
    "    \n",
    "    # update weights\n",
    "    with torch.no_grad(): # important: This is mandatory as we don't want to tie w with \n",
    "        w -= learningRate * w.grad;\n",
    "\n",
    "    w.grad.zero_(); # have to make the gradients zero again. Why though? cause the L.backward() line will keep accumulating w.grad if we don't set it to zero every time\n",
    "    \n",
    "    if epoch % 20 ==0:\n",
    "        print(f'epoch {epoch + 1}: w = {w:.3f}, loss = {L:0.3f}');\n",
    "print(f'Prediction after training: f(5) = {forward(5)}');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8f10c7-3e74-4252-b1aa-fa6a8e095452",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tutorial 6: Training pipeline: model, loss, and optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c286571-43a6-43ef-a997-95db0efd1f56",
   "metadata": {},
   "source": [
    "It turns out, Pytorch lets us replace the manually computed loss, parameter updates. Steps: \n",
    "\n",
    "1. Design model (input, output, forward pass)\n",
    "2. loss and optimizer\n",
    "3. Tranining loop:\n",
    "* forward pass: compute prediction\n",
    "* backward pass: gradients\n",
    "* update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c2601be4-8321-4655-aa40-d43928e8aec7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training : f(5) = -1.354038953781128\n",
      "epoch 1: w = -0.104, loss = 35.267\n",
      "epoch 21: w = 1.495, loss = 0.335\n",
      "epoch 41: w = 1.563, loss = 0.276\n",
      "epoch 61: w = 1.589, loss = 0.245\n",
      "epoch 81: w = 1.613, loss = 0.217\n",
      "Prediction after training: f(5) = tensor([[8.1729]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn \n",
    "\n",
    "learningRate = 0.01\n",
    "nIters = 100;\n",
    "\n",
    "X = torch.tensor([[1], [2], [3], [4]], dtype = torch.float32); # need the general case of nsamples and nfeatures so have to cast it into this form\n",
    "Y = torch.tensor([[2], [4], [6], [8]], dtype = torch.float32); \"\"\" these are our input data. we'll create a model that fits this data. That is find the optimum w\"\"\"\n",
    "\n",
    "X_test = torch.tensor([5], dtype = torch.float32);\n",
    "\n",
    "nSamples, nFeatures = X.shape;\n",
    "inputSize = nFeatures; outputSize = nFeatures;\n",
    "\n",
    "model = torch.nn.Linear(inputSize, outputSize)\n",
    "\n",
    "# loss and gradient can be automatically calculated (newly added)\n",
    "LossFun = torch.nn.MSELoss();\n",
    "optimizerFun = torch.optim.SGD(model.parameters(), lr=learningRate); \n",
    "\n",
    "print(f'Prediction before training : f(5) = {model(X_test).item()}');\n",
    "\n",
    "for epoch in range(nIters):\n",
    "    YHat = model(X);\n",
    "    \n",
    "    L = loss(Y, YHat);    \n",
    "    \n",
    "    L.backward(); \n",
    "    \n",
    "    optimizerFun.step();\n",
    "    \n",
    "    optimizerFun.zero_grad(); # emptying the gradients\n",
    "    \n",
    "    if epoch % 20 ==0:\n",
    "        [w, b] = model.parameters()\n",
    "        print(f'epoch {epoch + 1}: w = {w[0][0].item():.3f}, loss = {L:0.3f}');\n",
    "        \n",
    "print(f'Prediction after training: f(5) = {forward(5)}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "e71f7c37-c031-4dd5-87fe-76c95169af50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we can define the model more formally too like this\n",
    "class LinearRegression(torch.nn.Module):\n",
    "    def __init__(self, inputDim, outputDim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.lin = torch.nn.Linear(inputDim, outputDim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "    \n",
    "model = LinearRegression(inputSize, outputSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5f5546-2e8c-422c-9b16-d74c0861e30a",
   "metadata": {},
   "source": [
    "### Tutorial 7: Recap of Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5496bd-cc0c-4170-94c7-8b374fa239a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
